---
# Source: nebula-core/templates/admin/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nebulaadmin
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
  annotations: 
    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_NUMBER>:role/iam-role-nebula
---
# Source: nebula-core/templates/datacatalog/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: datacatalog
  namespace: nebula
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
  annotations: 
    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_NUMBER>:role/iam-role-nebula
---
# Source: nebula-core/templates/nebulascheduler/sa.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nebulascheduler
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulascheduler
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
---
# Source: nebula-core/templates/propeller/rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nebulapropeller
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulapropeller
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
  annotations: 
    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_NUMBER>:role/iam-role-nebula
---
# Source: nebula-core/templates/propeller/webhook.yaml
# Create a Service Account for webhook
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nebula-pod-webhook
  namespace: nebula
---
# Source: nebula-core/templates/admin/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: nebula-admin-secrets
  namespace: nebula
type: Opaque
stringData:
---
# Source: nebula-core/templates/common/secret-auth.yaml
apiVersion: v1
kind: Secret
metadata:
  name: nebula-secret-auth
  namespace: nebula
type: Opaque
stringData:
  client_secret: foobar
---
# Source: nebula-core/templates/common/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-pass
stringData:
  pass.txt: '<DB_PASSWORD>'
type: Opaque
---
# Source: nebula-core/templates/propeller/webhook.yaml
# Create an empty secret that the first propeller pod will populate
apiVersion: v1
kind: Secret
metadata:
  name: nebula-pod-webhook
  namespace: nebula
type: Opaque
---
# Source: nebula-core/templates/admin/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nebula-admin-clusters-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  clusters.yaml: |
    clusters:
      clusterConfigs: []
      labelClusterMap: {}
---
# Source: nebula-core/templates/admin/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nebula-admin-base-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  db.yaml: | 
    database:
      dbname: nebulaadmin
      host: '<RDS_HOST>'
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: nebulaadmin
  domain.yaml: | 
    domains:
    - id: development
      name: development
    - id: staging
      name: staging
    - id: production
      name: production
  server.yaml: | 
    auth:
      appAuth:
        thirdPartyConfig:
          nebulaClient:
            clientId: nebulactl
            redirectUri: http://localhost:53593/callback
            scopes:
            - offline
            - all
      authorizedUris:
      - https://localhost:30081
      - http://nebulaadmin:80
      - http://nebulaadmin.nebula.svc.cluster.local:80
      userAuth:
        openId:
          baseUrl: https://accounts.google.com
          clientId: 657465813211-6eog7ek7li5k7i7fvgv2921075063hpe.apps.googleusercontent.com
          scopes:
          - profile
          - openid
    nebulaadmin:
      eventVersion: 2
      metadataStoragePrefix:
      - metadata
      - admin
      metricsScope: 'nebula:'
      profilerPort: 10254
      roleNameKey: iam.amazonaws.com/role
      testing:
        host: http://nebulaadmin
    server:
      grpcPort: 8089
      httpPort: 8088
      security:
        allowCors: true
        allowedHeaders:
        - Content-Type
        allowedOrigins:
        - '*'
        secure: false
        useAuth: false
  remoteData.yaml: | 
    remoteData:
      region: us-east-1
      scheme: local
      signedUrls:
        durationMinutes: 3
  storage.yaml: | 
    storage:
      type: s3
      container: "<BUCKET_NAME>"
      connection:
        auth-type: iam
        region: <AWS_REGION>
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
  task_resource_defaults.yaml: | 
    task_resources:
      defaults:
        cpu: 1000m
        memory: 1000Mi
        storage: 1000Mi
      limits:
        cpu: 2
        gpu: 1
        memory: 1Gi
        storage: 2000Mi
  cluster_resources.yaml: | 
    cluster_resources:
      customData:
      - production:
        - projectQuotaCpu:
            value: "5"
        - projectQuotaMemory:
            value: 4000Mi
        - defaultIamRole:
            value: arn:aws:iam::<ACCOUNT_NUMBER>:role/nebula-user-role
      - staging:
        - projectQuotaCpu:
            value: "2"
        - projectQuotaMemory:
            value: 3000Mi
        - defaultIamRole:
            value: arn:aws:iam::<ACCOUNT_NUMBER>:role/nebula-user-role
      - development:
        - projectQuotaCpu:
            value: "4"
        - projectQuotaMemory:
            value: 3000Mi
        - defaultIamRole:
            value: arn:aws:iam::<ACCOUNT_NUMBER>:role/nebula-user-role
      refreshInterval: 5m
      standaloneDeployment: false
      templatePath: /etc/nebula/clusterresource/templates
---
# Source: nebula-core/templates/clusterresourcesync/cluster_resource_configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clusterresource-template
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  aa_namespace.yaml: | 
    apiVersion: v1
    kind: Namespace
    metadata:
      name: {{ namespace }}
    spec:
      finalizers:
      - kubernetes
    
  aab_default_service_account.yaml: | 
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: default
      namespace: {{ namespace }}
      annotations:
        eks.amazonaws.com/role-arn: {{ defaultIamRole }}
    
  ab_project_resource_quota.yaml: | 
    apiVersion: v1
    kind: ResourceQuota
    metadata:
      name: project-quota
      namespace: {{ namespace }}
    spec:
      hard:
        limits.cpu: {{ projectQuotaCpu }}
        limits.memory: {{ projectQuotaMemory }}
---
# Source: nebula-core/templates/clusterresourcesync/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nebula-clusterresourcesync-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  cluster_resources.yaml: | 
    cluster_resources:
      customData:
      - production:
        - projectQuotaCpu:
            value: "5"
        - projectQuotaMemory:
            value: 4000Mi
        - defaultIamRole:
            value: arn:aws:iam::<ACCOUNT_NUMBER>:role/nebula-user-role
      - staging:
        - projectQuotaCpu:
            value: "2"
        - projectQuotaMemory:
            value: 3000Mi
        - defaultIamRole:
            value: arn:aws:iam::<ACCOUNT_NUMBER>:role/nebula-user-role
      - development:
        - projectQuotaCpu:
            value: "4"
        - projectQuotaMemory:
            value: 3000Mi
        - defaultIamRole:
            value: arn:aws:iam::<ACCOUNT_NUMBER>:role/nebula-user-role
      refreshInterval: 5m
      standaloneDeployment: false
      templatePath: /etc/nebula/clusterresource/templates
  db.yaml: | 
    database:
      dbname: nebulaadmin
      host: '<RDS_HOST>'
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: nebulaadmin
  domain.yaml: | 
    domains:
    - id: development
      name: development
    - id: staging
      name: staging
    - id: production
      name: production
  clusters.yaml: |
    clusters:
      clusterConfigs: []
      labelClusterMap: {}
---
# Source: nebula-core/templates/console/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nebula-console-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaconsole
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data: 
  BASE_URL: /console
  CONFIG_DIR: /etc/nebula/config
---
# Source: nebula-core/templates/datacatalog/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: datacatalog-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  db.yaml: | 
    database:
      dbname: nebulaadmin
      host: '<RDS_HOST>'
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: nebulaadmin
  server.yaml: | 
    application:
      grpcPort: 8089
      grpcServerReflection: true
      httpPort: 8080
    datacatalog:
      heartbeat-grace-period-multiplier: 3
      max-reservation-heartbeat: 30s
      metrics-scope: datacatalog
      profiler-port: 10254
      storage-prefix: metadata/datacatalog
  storage.yaml: | 
    storage:
      type: s3
      container: "<BUCKET_NAME>"
      connection:
        auth-type: iam
        region: <AWS_REGION>
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
---
# Source: nebula-core/templates/nebulascheduler/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nebula-scheduler-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulascheduler
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  admin.yaml: | 
    admin:
      clientId: 'nebulapropeller'
      clientSecretLocation: /etc/secrets/client_secret
      endpoint: nebulaadmin:81
      insecure: true
    event:
      capacity: 1000
      rate: 500
      type: admin
  db.yaml: | 
    database:
      dbname: nebulaadmin
      host: '<RDS_HOST>'
      passwordPath: /etc/db/pass.txt
      port: 5432
      username: nebulaadmin
  server.yaml: | 
    scheduler:
      metricsScope: 'nebula:'
      profilerPort: 10254
---
# Source: nebula-core/templates/propeller/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nebula-propeller-config
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
data:
  admin.yaml: | 
    admin:
      clientId: 'nebulapropeller'
      clientSecretLocation: /etc/secrets/client_secret
      endpoint: nebulaadmin:81
      insecure: true
    event:
      capacity: 1000
      rate: 500
      type: admin
  catalog.yaml: | 
    catalog-cache:
      endpoint: datacatalog:89
      insecure: true
      type: datacatalog
  copilot.yaml: | 
    plugins:
      k8s:
        co-pilot:
          image: cr.nebula.org/nebulaclouds/nebulacopilot:v1.10.6
          name: nebula-copilot-
          start-timeout: 30s
  core.yaml: | 
    manager:
      pod-application: nebulapropeller
      pod-template-container-name: nebulapropeller
      pod-template-name: nebulapropeller-template
    propeller:
      downstream-eval-duration: 30s
      enable-admin-launcher: true
      gc-interval: 12h
      kube-client-config:
        burst: 25
        qps: 100
        timeout: 30s
      leader-election:
        enabled: true
        lease-duration: 15s
        lock-config-map:
          name: propeller-leader
          namespace: nebula
        renew-deadline: 10s
        retry-period: 2s
      limit-namespace: all
      max-workflow-retries: 50
      metadata-prefix: metadata/propeller
      metrics-prefix: nebula
      prof-port: 10254
      queue:
        batch-size: -1
        batching-interval: 2s
        queue:
          base-delay: 5s
          capacity: 1000
          max-delay: 120s
          rate: 100
          type: maxof
        sub-queue:
          capacity: 1000
          rate: 100
          type: bucket
        type: batch
      rawoutput-prefix: s3://<BUCKET_NAME>/
      workers: 40
      workflow-reeval-duration: 30s
    webhook:
      certDir: /etc/webhook/certs
      serviceName: nebula-pod-webhook
  enabled_plugins.yaml: | 
    tasks:
      task-plugins:
        default-for-task-types:
          container: container
          container_array: k8s-array
          sidecar: sidecar
        enabled-plugins:
        - container
        - sidecar
        - k8s-array
  k8s.yaml: | 
    plugins:
      k8s:
        default-cpus: 100m
        default-env-vars: []
        default-memory: 100Mi
  resource_manager.yaml: | 
    propeller:
      resourcemanager:
        type: noop
  storage.yaml: | 
    storage:
      type: s3
      container: "<BUCKET_NAME>"
      connection:
        auth-type: iam
        region: <AWS_REGION>
      enable-multicontainer: false
      limits:
        maxDownloadMBs: 10
  cache.yaml: |
    cache:
      max_size_mbs: 1024
      target_gc_percent: 70
  task_logs.yaml: | 
    plugins:
      logs:
        cloudwatch-enabled: true
        cloudwatch-log-group: '<LOG_GROUP_NAME>'
        cloudwatch-region: '<AWS_REGION>'
        kubernetes-enabled: false
---
# Source: nebula-core/templates/propeller/crds/nebulaworkflow.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: nebulaworkflows.nebula.lyft.com
spec:
  group: nebula.lyft.com
  names:
    kind: NebulaWorkflow
    plural: nebulaworkflows
    shortNames:
      - fly
    singular: nebulaworkflow
  scope: Namespaced
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          x-kubernetes-preserve-unknown-fields: true
          properties:
---
# Source: nebula-core/templates/admin/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nebula-nebulaadmin
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups: 
  - ""
  - nebula.lyft.com
  - rbac.authorization.k8s.io
  resources: 
  - configmaps
  - nebulaworkflows
  - namespaces
  - pods
  - resourcequotas
  - roles
  - rolebindings
  - secrets
  - services
  - serviceaccounts
  - spark-role
  - limitranges
  verbs: 
  - '*'
---
# Source: nebula-core/templates/propeller/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: nebula-nebulapropeller
  labels: 
    app.kubernetes.io/name: nebulapropeller
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
rules:
# Allow RO access to PODS
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
  - list
  - watch
# Allow Event recording access
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - update
  - delete
  - patch
# Allow Access All plugin objects
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
  - patch
# Allow Access to CRD
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - update
# Allow Access to all resources under nebula.lyft.com
- apiGroups:
  - nebula.lyft.com
  resources:
  - nebulaworkflows
  - nebulaworkflows/finalizers
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
  - patch
  - post
  - deletecollection
---
# Source: nebula-core/templates/propeller/webhook.yaml
# Create a ClusterRole for the webhook
# https://kubernetes.io/docs/admin/authorization/rbac/
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nebula-nebula-pod-webhook
  namespace: nebula
rules:
  - apiGroups:
      - "*"
    resources:
      - mutatingwebhookconfigurations
      - secrets
      - pods
      - replicasets/finalizers
    verbs:
      - get
      - create
      - update
      - patch
---
# Source: nebula-core/templates/admin/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nebula-nebulaadmin-binding
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nebula-nebulaadmin
subjects:
- kind: ServiceAccount
  name: nebulaadmin
  namespace: nebula
---
# Source: nebula-core/templates/propeller/rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: nebula-nebulapropeller
  labels: 
    app.kubernetes.io/name: nebulapropeller
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nebula-nebulapropeller
subjects:
- kind: ServiceAccount
  name: nebulapropeller
  namespace: nebula
---
# Source: nebula-core/templates/propeller/webhook.yaml
# Create a binding from Role -> ServiceAccount
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nebula-nebula-pod-webhook
  namespace: nebula
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: nebula-nebula-pod-webhook
subjects:
  - kind: ServiceAccount
    name: nebula-pod-webhook
    namespace: nebula
---
# Source: nebula-core/templates/admin/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nebulaadmin
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 8088
    - name: grpc
      port: 81
      protocol: TCP
      targetPort: 8089
    - name: redoc
      protocol: TCP
      port: 87
      targetPort: 8087
    - name: http-metrics
      protocol: TCP
      port: 10254
  selector: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
---
# Source: nebula-core/templates/console/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: nebulaconsole
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaconsole
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  selector: 
    app.kubernetes.io/name: nebulaconsole
    app.kubernetes.io/instance: nebula
---
# Source: nebula-core/templates/datacatalog/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: datacatalog
  namespace: nebula
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  type: NodePort
  ports:
  - name: http
    port: 88
    protocol: TCP
    targetPort: 8088
  - name: grpc
    port: 89
    protocol: TCP
    targetPort: 8089
  selector: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: nebula
---
# Source: nebula-core/templates/propeller/webhook.yaml
# Service
apiVersion: v1
kind: Service
metadata:
  name: nebula-pod-webhook
  namespace: nebula
  annotations: 
    projectcontour.io/upstream-protocol.h2c: grpc
spec:
  selector:
    app: nebula-pod-webhook
  ports:
    - name: https
      protocol: TCP
      port: 443
      targetPort: 9443
---
# Source: nebula-core/templates/admin/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nebulaadmin
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaadmin
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels: 
      app.kubernetes.io/name: nebulaadmin
      app.kubernetes.io/instance: nebula
  template:
    metadata:
      annotations:
        configChecksum: "053b20ebc40227f6ed8ddc61f5997ee7997c604158f773779f20ec61af11a2f"
      labels: 
        app.kubernetes.io/name: nebulaadmin
        app.kubernetes.io/instance: nebula
        helm.sh/chart: nebula-core-v0.1.10
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        fsGroup: 65534
        runAsUser: 1001
        fsGroupChangePolicy: "Always"
      initContainers:
        - command:
          - nebulaadmin
          - --config
          - /etc/nebula/config/*.yaml
          - migrate
          - run
          image: "cr.nebula.org/nebulaclouds/nebulaadmin:v1.10.6"
          imagePullPolicy: "IfNotPresent"
          name: run-migrations
          volumeMounts:
          - mountPath: /etc/db
            name: db-pass
          - mountPath: /etc/nebula/config
            name: base-config-volume
        - command:
          - nebulaadmin
          - --config
          - /etc/nebula/config/*.yaml
          - migrate
          - seed-projects
          - nebulasnacks
          - nebulatester
          - nebulaexamples
          image: "cr.nebula.org/nebulaclouds/nebulaadmin:v1.10.6"
          imagePullPolicy: "IfNotPresent"
          name: seed-projects
          volumeMounts:
          - mountPath: /etc/db
            name: db-pass
          - mountPath: /etc/nebula/config
            name: base-config-volume
        - command:
          - nebulaadmin
          - --config
          - /etc/nebula/config/*.yaml
          - clusterresource
          - sync
          image: "cr.nebula.org/nebulaclouds/nebulaadmin:v1.10.6"
          imagePullPolicy: "IfNotPresent"
          name: sync-cluster-resources
          volumeMounts:
          - mountPath: /etc/db
            name: db-pass
          - mountPath: /etc/nebula/clusterresource/templates
            name: resource-templates
          - mountPath: /etc/nebula/config
            name: clusters-config-volume
          - mountPath: /etc/secrets/
            name: admin-secrets
        - name: generate-secrets
          image: "cr.nebula.org/nebulaclouds/nebulaadmin:v1.10.6"
          imagePullPolicy: "IfNotPresent"
          command: ["/bin/sh", "-c"]
          args:
            [
                "nebulaadmin --config=/etc/nebula/config/*.yaml secrets init --localPath /etc/scratch/secrets && nebulaadmin --config=/etc/nebula/config/*.yaml secrets create --name nebula-admin-secrets --fromPath /etc/scratch/secrets",
            ]
          volumeMounts:
            - mountPath: /etc/nebula/config
              name: base-config-volume
            - mountPath: /etc/scratch
              name: scratch
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      containers:
      - command:
        - nebulaadmin
        - --config
        - /etc/nebula/config/*.yaml
        - serve
        image: "cr.nebula.org/nebulaclouds/nebulaadmin:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        name: nebulaadmin
        ports:
        - containerPort: 8088
        - containerPort: 8089
        - containerPort: 10254
        readinessProbe:
          exec:
            command: [ "sh", "-c", "reply=$(curl -s -o /dev/null -w %{http_code} http://127.0.0.1:8088/healthcheck); if [ \"$reply\" -lt 200 -o \"$reply\" -ge 400 ]; then exit 1; fi;","grpc_health_probe", "-addr=:8089"]
          initialDelaySeconds: 15
        livenessProbe:
          exec:
            command: [ "sh", "-c", "reply=$(curl -s -o /dev/null -w %{http_code} http://127.0.0.1:8088/healthcheck); if [ \"$reply\" -lt 200 -o \"$reply\" -ge 400 ]; then exit 1; fi;","grpc_health_probe", "-addr=:8089"]
          initialDelaySeconds: 20
          periodSeconds: 5
        resources:
          limits:
            cpu: 250m
            ephemeral-storage: 200Mi
            memory: 500Mi
          requests:
            cpu: 50m
            ephemeral-storage: 200Mi
            memory: 200Mi
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /srv/nebula
          name: shared-data
        - mountPath: /etc/nebula/config
          name: clusters-config-volume
        - mountPath: /etc/secrets/
          name: admin-secrets
      serviceAccountName: nebulaadmin
      volumes:
      - name: db-pass
        secret:
          secretName: db-pass
      - emptyDir: {}
        name: shared-data
      - emptyDir: {}
        name: scratch
      - configMap:
          name: nebula-admin-base-config
        name: base-config-volume
      - projected:
          sources:
            - configMap:
                name: nebula-admin-base-config
            - configMap:
                name: nebula-admin-clusters-config
        name: clusters-config-volume
      - configMap:
          name: clusterresource-template
        name: resource-templates
      - name: admin-secrets
        secret:
          secretName: nebula-admin-secrets
      affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: nebulaadmin
            topologyKey: kubernetes.io/hostname
---
# Source: nebula-core/templates/clusterresourcesync/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: syncresources
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaclusterresourcesync
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: nebulaclusterresourcesync
      app.kubernetes.io/instance: nebula
  template:
    metadata:
      annotations:
        configChecksum: "55ce597c10b17ef6e891f0c9242b17aafb3d7b4e4e414d0a5078d71ad9c804f"
      labels: 
        app.kubernetes.io/name: nebulaclusterresourcesync
        app.kubernetes.io/instance: nebula
        helm.sh/chart: nebula-core-v0.1.10
        app.kubernetes.io/managed-by: Helm
    spec:
      containers:
        - command:
            - nebulaadmin
            - --config
            - /etc/nebula/config/*.yaml
            - clusterresource
            - run
          image: "cr.nebula.org/nebulaclouds/nebulaadmin:v1.10.6"
          imagePullPolicy: "IfNotPresent"
          name: sync-cluster-resources
          volumeMounts:
          - mountPath: /etc/db
            name: db-pass
          - mountPath: /etc/nebula/clusterresource/templates
            name: resource-templates
          - mountPath: /etc/nebula/config
            name: config-volume
      serviceAccountName: nebulaadmin
      volumes:
        - name: db-pass
          secret:
            secretName: db-pass
        - configMap:
            name: clusterresource-template
          name: resource-templates
        - configMap:
            name: nebula-clusterresourcesync-config
          name: config-volume
---
# Source: nebula-core/templates/console/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nebulaconsole
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulaconsole
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels: 
      app.kubernetes.io/name: nebulaconsole
      app.kubernetes.io/instance: nebula
  template:
    metadata:
      annotations:
        configChecksum: "2f930e1732c47d0849f79f9a8d06262ec97597a217bbf2337ae4f2938402ee0"
      labels: 
        app.kubernetes.io/name: nebulaconsole
        app.kubernetes.io/instance: nebula
        helm.sh/chart: nebula-core-v0.1.10
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        runAsUser: 1000
        fsGroupChangePolicy: "OnRootMismatch"
      containers:
      - image: "cr.nebula.org/nebulaclouds/nebulaconsole:v1.10.2"
        imagePullPolicy: "IfNotPresent"
        name: nebulaconsole
        envFrom:
        - configMapRef:
            name: nebula-console-config
        ports:
        - containerPort: 8080
        resources: 
          limits:
            cpu: 250m
            memory: 250Mi
          requests:
            cpu: 10m
            memory: 50Mi
        volumeMounts:
        - mountPath: /srv/nebula
          name: shared-data
      volumes:
      - emptyDir: {}
        name: shared-data
      affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: nebulaconsole
            topologyKey: kubernetes.io/hostname
---
# Source: nebula-core/templates/datacatalog/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: datacatalog
  namespace: nebula
  labels: 
    app.kubernetes.io/name: datacatalog
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels: 
      app.kubernetes.io/name: datacatalog
      app.kubernetes.io/instance: nebula
  template:
    metadata:
      annotations:
        configChecksum: "59ef5b555bd41c3e854a315f21031c76dfa876455ff8069b989cb6c28ec1f17"
      labels: 
        app.kubernetes.io/name: datacatalog
        app.kubernetes.io/instance: nebula
        helm.sh/chart: nebula-core-v0.1.10
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        fsGroup: 1001
        runAsUser: 1001
        fsGroupChangePolicy: "OnRootMismatch"
      initContainers:
      - command:
        - datacatalog
        - --config
        - /etc/datacatalog/config/*.yaml
        - migrate
        - run
        image: "cr.nebula.org/nebulaclouds/datacatalog:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        name: run-migrations
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/datacatalog/config
          name: config-volume
      containers:
      - command:
        - datacatalog
        - --config
        - /etc/datacatalog/config/*.yaml
        - serve
        image: "cr.nebula.org/nebulaclouds/datacatalog:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        name: datacatalog
        ports:
        - containerPort: 8088
        - containerPort: 8089
        - containerPort: 10254
        resources:
          limits:
            cpu: 1
            ephemeral-storage: 200Mi
            memory: 500Mi
          requests:
            cpu: 500m
            ephemeral-storage: 200Mi
            memory: 200Mi
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/datacatalog/config
          name: config-volume
      serviceAccountName: datacatalog
      volumes:
      - name: db-pass
        secret:
          secretName: db-pass
      - emptyDir: {}
        name: shared-data
      - configMap:
          name: datacatalog-config
        name: config-volume
      affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: datacatalog
            topologyKey: kubernetes.io/hostname
---
# Source: nebula-core/templates/nebulascheduler/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nebulascheduler
  namespace: nebula
  labels: 
    app.kubernetes.io/name: nebulascheduler
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: nebulascheduler
      app.kubernetes.io/instance: nebula
  template:
    metadata:
      annotations:
        configChecksum: "053b20ebc40227f6ed8ddc61f5997ee7997c604158f773779f20ec61af11a2f"
      labels: 
        app.kubernetes.io/name: nebulascheduler
        app.kubernetes.io/instance: nebula
        helm.sh/chart: nebula-core-v0.1.10
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        fsGroup: 65534
        runAsUser: 1001
        fsGroupChangePolicy: "Always"
      initContainers:
      - command:
        - nebulascheduler
        - precheck
        - --config
        - /etc/nebula/config/*.yaml
        image: "cr.nebula.org/nebulaclouds/nebulascheduler:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        name: nebulascheduler-check
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/nebula/config
          name: config-volume
        - name: auth
          mountPath: /etc/secrets/
      containers:
      - command:
        - nebulascheduler
        - run
        - --config
        - /etc/nebula/config/*.yaml
        image: "cr.nebula.org/nebulaclouds/nebulascheduler:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        name: nebulascheduler
        ports:
          - containerPort: 10254
        resources:
          limits:
            cpu: 250m
            ephemeral-storage: 100Mi
            memory: 500Mi
          requests:
            cpu: 10m
            ephemeral-storage: 50Mi
            memory: 50Mi
        volumeMounts:
        - mountPath: /etc/db
          name: db-pass
        - mountPath: /etc/nebula/config
          name: config-volume
        - name: auth
          mountPath: /etc/secrets/
      serviceAccountName: nebulascheduler
      volumes:
      - name: db-pass
        secret:
          secretName: db-pass
      - emptyDir: {}
        name: shared-data
      - configMap:
          name: nebula-scheduler-config
        name: config-volume
      - name: auth
        secret:
          secretName: nebula-secret-auth
---
# Source: nebula-core/templates/propeller/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: nebula
  name: nebulapropeller
  labels: 
    app.kubernetes.io/name: nebulapropeller
    app.kubernetes.io/instance: nebula
    helm.sh/chart: nebula-core-v0.1.10
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 2
  selector:
    matchLabels: 
      app.kubernetes.io/name: nebulapropeller
      app.kubernetes.io/instance: nebula
  template:
    metadata:
      annotations:
        configChecksum: "fd3294c2987d4c2863e13a84abd65a284f69332d97a698c833c27e5c9f5417a"
      labels: 
        app.kubernetes.io/name: nebulapropeller
        app.kubernetes.io/instance: nebula
        helm.sh/chart: nebula-core-v0.1.10
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        fsGroup: 65534
        runAsUser: 1001
        fsGroupChangePolicy: "Always"
      priorityClassName: system-cluster-critical
      containers:
      - command:
        - nebulapropeller
        - --config
        - /etc/nebula/config/*.yaml
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: "cr.nebula.org/nebulaclouds/nebulapropeller:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        name: nebulapropeller
        ports:
        - containerPort: 10254
        resources:
          limits:
            cpu: 1
            ephemeral-storage: 1Gi
            memory: 2Gi
          requests:
            cpu: 1
            ephemeral-storage: 1Gi
            memory: 2Gi
        volumeMounts:
        - name: config-volume
          mountPath: /etc/nebula/config
        - name: auth
          mountPath: /etc/secrets/
        terminationMessagePolicy: "FallbackToLogsOnError"
      serviceAccountName: nebulapropeller
      volumes:
      - configMap:
          name: nebula-propeller-config
        name: config-volume
      - name: auth
        secret:
          secretName: nebula-secret-auth
      affinity: 
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                app.kubernetes.io/name: nebulapropeller
            topologyKey: kubernetes.io/hostname
---
# Source: nebula-core/templates/propeller/webhook.yaml
# Create the actual deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nebula-pod-webhook
  namespace: nebula
  labels:
    app: nebula-pod-webhook
spec:
  selector:
    matchLabels:
      app: nebula-pod-webhook
  template:
    metadata:
      labels:
        app: nebula-pod-webhook
        app.kubernetes.io/name: nebula-pod-webhook
        app.kubernetes.io/version: v1.10.6
      annotations:
        configChecksum: "fd3294c2987d4c2863e13a84abd65a284f69332d97a698c833c27e5c9f5417a"
    spec:
      securityContext:
        fsGroup: 65534
        runAsUser: 1001
        fsGroupChangePolicy: "Always"
      serviceAccountName: nebula-pod-webhook
      initContainers:
      - name: generate-secrets
        image: "cr.nebula.org/nebulaclouds/nebulapropeller:v1.10.6"
        imagePullPolicy: "IfNotPresent"
        command:
          - nebulapropeller
        args:
          - webhook
          - init-certs
          - --config
          - /etc/nebula/config/*.yaml
        env:
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
        volumeMounts:
          - name: config-volume
            mountPath: /etc/nebula/config
      containers:
        - name: webhook
          image: "cr.nebula.org/nebulaclouds/nebulapropeller:v1.10.6"
          imagePullPolicy: "IfNotPresent"
          command:
            - nebulapropeller
          args:
            - webhook
            - --config
            - /etc/nebula/config/*.yaml
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          volumeMounts:
            - name: config-volume
              mountPath: /etc/nebula/config
              readOnly: true
            - name: webhook-certs
              mountPath: /etc/webhook/certs
              readOnly: true
      volumes:
        - name: config-volume
          configMap:
            name: nebula-propeller-config
        - name: webhook-certs
          secret:
            secretName: nebula-pod-webhook
---
# Source: nebula-core/templates/common/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nebula-core
  namespace: nebula
  annotations: 
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig":
      { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
    alb.ingress.kubernetes.io/certificate-arn: '<CERTIFICATE_ARN>'
    alb.ingress.kubernetes.io/group.name: nebula
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/tags: service_instance=production
    alb.ingress.kubernetes.io/target-type: ip
    kubernetes.io/ingress.class: alb
    nginx.ingress.kubernetes.io/app-root: /console
spec:
  rules:
    - http:
        paths:
          - path: /*
            pathType: ImplementationSpecific
            backend:
              service:
                name: ssl-redirect
                port:
                  name: use-annotation
          # This is useful only for frontend development
          # NOTE: If you change this, you must update the BASE_URL value in nebulaconsole.yaml
          - path: /console
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaconsole
                port:
                  number: 80
          - path: /console/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaconsole
                port:
                  number: 80
          - path: /api
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /api/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /healthcheck
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /v1/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /.well-known
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /.well-known/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /login
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /login/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /logout
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /logout/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /callback
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /callback/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /me
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /config
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /config/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /oauth2
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
          - path: /oauth2/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 80
      host: null
# Certain ingress controllers like nginx cannot serve HTTP 1 and GRPC with a single ingress because GRPC can only
# enabled on the ingress object, not on backend services (GRPC annotation is set on the ingress, not on the services).
---
# Source: nebula-core/templates/common/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: nebula-core-grpc
  namespace: nebula
  annotations:
    alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig":
      { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
    alb.ingress.kubernetes.io/certificate-arn: '<CERTIFICATE_ARN>'
    alb.ingress.kubernetes.io/group.name: nebula
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/tags: service_instance=production
    alb.ingress.kubernetes.io/target-type: ip
    kubernetes.io/ingress.class: alb
    nginx.ingress.kubernetes.io/app-root: /console
    alb.ingress.kubernetes.io/backend-protocol-version: GRPC
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
spec:
  rules:
    - host: null
      http:
        paths:
          #
          # - backend:
          #     serviceName: ssl-redirect
          #     servicePort: use-annotation
          #   path: /*
          #   pathType: ImplementationSpecific
          #
          
          # NOTE: Port 81 in nebulaadmin is the GRPC server port for NebulaAdmin.
          - path: /nebulaidl.service.SignalService
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.SignalService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.AdminService
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.AdminService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.DataProxyService
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.DataProxyService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.AuthMetadataService
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.AuthMetadataService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.IdentityService
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /nebulaidl.service.IdentityService/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /grpc.health.v1.Health
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
          - path: /grpc.health.v1.Health/*
            pathType: ImplementationSpecific
            backend:
              service:
                name: nebulaadmin
                port:
                  number: 81
