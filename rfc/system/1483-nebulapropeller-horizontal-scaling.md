# [RFC] NebulaPropeller Horizontal Scaling

**Authors:**

- @kumare3
- @hamersaw

This Document - https://hackmd.io/uVT5Q9zNS8SNMkEhs5pfqQ

## 1 Executive Summary

We propose an approach to facilitate automatic, horizontal scaling of NebulaPropeller. This is important to combat increases in individual workflow execution latency as the number of concurrent workflow executions increases. The solution includes a new NebulaPropeller Manager component which handles automatic scaling of additional NebulaPropeller instances. Additionally, we rely on a configurable sharding mechanism to ensure deterministic, decentralized coordination of Nebula workflows.

## 2 Motivation

NebulaPropeller is the engine that drives Nebula workflows. It is highly optimized and a single instance can run thousands of concurrent workflows. Internally, workflows are processed in a control loop which operates over a work queue. Consequently, as the number of concurrent workflows increases beyond a reasonable threshold, the latency of workflow completion will experience marginal increases as well.

The default NebulaPropeller deployment strategy is a single instance and while it is possible to launch multiple NebulaPropeller instances (ie. one per namespace) this has to be done manually. This approach has obvious limits to scalability. Therefore, to reduce workflow processing latency as the number of workflows increase, we need a new solution to increase the refresh rate per workflow.

In addition to the obvious scaling of workflow processing workloads, the system should be automated and should be self healing. It should have the following Safety and Liveness properties
- Safety properties
    - Every workflow should be executed: Once a workflow is being executed, it should continue to make progress
    - A workflow should not be lost, unless the k8s cluster is lost
    - The K8s API server should be safe
- Liveness properties
    - Once a Workflow starts executing, it should eventually either fail or succeed
    - As the number of workflows increase, existing workflows should reach completion
    - System should scale automatically and in case of failures should recover in a correct way - workflows should not get corrupted.
    - Correlated failures should be unlikely


## 3 Proposed Implementation

![](https://i.imgur.com/ru21IDU.png)

The proposed system intends to extend NebulaPropeller to add a new mode called propeller-manager. When NebulaPropeller is run in the manager mode, it can consume an additional configuration called the ShardingStrategy.

```yaml=
ShardingStrategy:
     Type: <name of strategy>
     Strategy: <strategy specific configuration>
```

Depending on the Sharding strategy the propeller-manager, launches one or more NebulaPropeller instances. Ideally the NebulaPropeller instances are identical to the propeller-manager (ie. k8s pod configuration) with minor changes that affect how the sharding works. The configuration will include NebulaPropeller image definition, serviceAccounts, etc.

```yaml=
ConstantShardingStrategy:
    Replicas: 4
```

To begin, we want to by implement a ConstantShardingStrategy, where the distribution mechanism is similar to consistent hashing. NebulaPropeller maintains a constant ring-size and NebulaAdmin submits CRD's with shard-bin tokens within that range. Specifically, NebulaAdmin assigns new workflow CRD's the following extra labels:

1. A shard-bin field with a number between \[0,256)
2. A project label and a domain label

The ConstantShardingStrategy configuration defines the number of NebulaPropeller instances (ie. Replicas) that NebulaPropeller Manager runs. Each replia computes its shard bin range using an assigned rank and the constant population size using the following algorithm. 

    [ 256*n/p, 256*(n+1)/p), where n = rank and p = population size

For example, with population = 4 and rank = 0 the range is \[0, 64), rank = 1 range is \[64, 128). Using this range as a template, NebulaPropeller replicas are able to use k8s server side [label selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/) to filter workflows, reducing constraints on bandwidth.

NebulaPropeller Manager will deploy replicas using k8s pods with deterministic naming. Because k8s resources are idempotent, attempts to create multiple instances the same ranked NebulaPropeller replica (ie. based on name) will be rejected. Additionally, if a replica fails NebulaPropeller Manager will automatically restart it, this functionality can me modeled as a control loop.

```go=
initialize p (population size)
while True:
    replicas = fetch_current_state
    if len(replicas) < p:
        missing_replicas = find_missing_replica()
        for r in missing_replicas:
            start(r)
```

**Shard System Extensibility:** Given the above setup we can extend the Sharding system to allow multiple strategies like - Dynamic load based sharding, namespace sharding, domain sharding etc. But we will start with the simplest strategy as this is the easiest to manage


## 4 Metrics & Dashboards

- As the number of workflows increase, the latency of workflow state transitions should remain constant or within a pre-determined bound.

## 5 Drawbacks

Support for additional sharding pardigms may require coordinated changes in NebulaPropeller and NebulaAdmin. For example:
1. Sharding strategies that do not comply with the proposed bin-based scheme (ex. namespace / domain sharding) require NebulaAdmin to include additional CRD values
2. Support for dynamic load-balanced sharding requires resource utilization metrics to be reported to NebulaAdmin

## 6 Alternatives

**Assigning Workflow Replica in NebulaPropeller Manager:** We could have simply assigned the workflows to a replica in the manager, but this has a performance overhead, where the manager process has to consume each workflow CRD and then apply the label for the bin size. Instead of this, moving the labeling to be a constant ring-size in NebulaAdmin reduces overhead with the following advantages:
- Reduces Acceptance latency (startup latency). This is a very critical metric for Nebula users. This is because we do not need to assign this value as an extra step in the nebulapropeller-manager.
- Since the bins are statically assigned as labels, Nebulapropeller can use server side label queries to filter the workflows that are retrieved by every instance. This reduces the total workflows that need to be processed and reduces constraints on bandwidth. Read more about it in Labels and Selectors.
- Improves debuggability of the system.

**Using Leases to Assign Replica Rank:** Each NebulaPropeller instance gets the population size as an input or queries k8s for its replica count. With a known p value, each NebulaPropeller now tries to acquire a lease - using the k8s lease object. Each NebulaPropeller instance does not know which leases are available, so it can either test all leases, or try to acquire a lease at random or acquire them in serial order. Lease acquisition in the absence of a true-time or known clock uses a bounded-drift style algorithm, which delays a NebulaPropeller instanceâ€™s lease acquisition time and this slows down the startup. The slowdown is directly proportional to the number of failed lease acquisition attempts and is not deterministic.
A much simpler model is to use a central controller, which is a leader elected single process and hence can take decisions like a control loop. This closely resembles K8s mode and is simpler to implement - relatively bug-free.

**Using Deployment Directly:** The biggest challenge with using deployment is that each propeller needs to know its rank. Deployments do not currently have a way to pass this in, even as an environment variable. To achieve this, we will have to do 2 additional things which complicate the system especially during re-balancing and recovery from failure.
1. Listen to the deployment and look at the replicas to determine p
2. Come up with a dynamic algorithm that tries to acquire different ranks or ranks in order.


## 7 Potential Impact and Dependencies

Existing deployment of NebulaPropeller should continue to run without interruptions. If users wish to upgrade their NebulaPropeller deployments - then it should be as simple as changing the execution command or a configuration.

## 8 Unresolved questions

_NA_

## 9 Conclusion

The solution presented in this document facilitates efficient horizontal scaling of NebulaPropeller. This will effectively reduce the latency of individual workflow execution as the number of concurrent workflow executions increase. The approach presents an extensible, decentralized solution in adherence with existing k8s principles.