configuration:
  database:
    password: <DB_PASSWORD>
    host: <RDS_HOST_DNS>
    dbname: app
  storage:
    metadataContainer: <BUCKET_NAME>
    userDataContainer: <USER_DATA_BUCKET_NAME>
    provider: s3
    providerConfig:
      s3:
        region: "us-east-2"
        authType: "iam"
  logging:
    level: 5
    plugins:
      cloudwatch:
        enabled: true
        templateUri: |-
          https://console.aws.amazon.com/cloudwatch/home?region=<AWS_REGION>#logEventViewer:group=/eks/opta-development/cluster;stream=var.log.containers.{{ .podName }}_{{ .namespace }}_{{ .containerName }}-{{ .containerId }}.log
  auth:
    enabled: true
    oidc:
      baseUrl: https://signin.hosted.unionai.cloud/oauth2/default
      clientId: <IDP_CLIENT_ID>
      clientSecret: <IDP_CLIENT_SECRET>
    internal:
      clientSecret: <CC_PASSWD>
      clientSecretHash: <HASHED_CC_PASSWD>
    authorizedUris:
    - https://nebula.company.com
  inline:
    cluster_resources:
      customData:
      - production:
        - defaultIamRole:
            value: <NEBULA_USER_IAM_ARN>
      - staging:
        - defaultIamRole:
            value: <NEBULA_USER_IAM_ARN>
      - development:
        - defaultIamRole:
            value: <NEBULA_USER_IAM_ARN>
    nebulaadmin:
      roleNameKey: "iam.amazonaws.com/role"
    plugins:
      k8s:
        inject-finalizer: true
        default-env-vars:
          - AWS_METADATA_SERVICE_TIMEOUT: 5
          - AWS_METADATA_SERVICE_NUM_ATTEMPTS: 20
      spark:
        spark-config-default:
          - spark.hadoop.fs.s3a.aws.credentials.provider: com.amazonaws.auth.DefaultAWSCredentialsProviderChain
          - spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: "2"
          - spark.kubernetes.allocation.batch.size: "50"
          - spark.hadoop.fs.s3a.acl.default: BucketOwnerFullControl
          - spark.hadoop.fs.s3n.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
          - spark.hadoop.fs.AbstractFileSystem.s3n.impl: org.apache.hadoop.fs.s3a.S3A
          - spark.hadoop.fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
          - spark.hadoop.fs.AbstractFileSystem.s3.impl: org.apache.hadoop.fs.s3a.S3A
          - spark.hadoop.fs.s3a.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
          - spark.hadoop.fs.AbstractFileSystem.s3a.impl: org.apache.hadoop.fs.s3a.S3A
          - spark.hadoop.fs.s3a.multipart.threshold: "536870912"
          - spark.blacklist.enabled: "true"
          - spark.blacklist.timeout: 5m
          - spark.task.maxfailures: "8"
    storage:
      cache:
        max_size_mbs: 10
        target_gc_percent: 100
    tasks:
      task-plugins:
        enabled-plugins:
          - container
          - sidecar
          - K8S-ARRAY
          - spark
        default-for-task-types:
          - container: container
          - container_array: K8S-ARRAY
          - spark: spark
clusterResourceTemplates:
  inline:
    001_namespace.yaml: |
      apiVersion: v1
      kind: Namespace
      metadata:
        name: '{{ namespace }}'
    010_spark_role.yaml: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      metadata:
        name: spark-role
        namespace: '{{ namespace }}'
      rules:
      - apiGroups:
        - ""
        resources:
        - pods
        - services
        - configmaps
        verbs:
        - '*'
    011_spark_service_account.yaml: |
      apiVersion: v1
      kind: ServiceAccount
      metadata:
        name: spark
        namespace: '{{ namespace }}'
        annotations:
          eks.amazonaws.com/role-arn: '{{ defaultIamRole }}'
    012_spark_role_binding.yaml: |
      apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      metadata:
        name: spark-role-binding
        namespace: '{{ namespace }}'
      roleRef:
        apiGroup: rbac.authorization.k8s.io
        kind: Role
        name: spark-role
      subjects:
      - kind: ServiceAccount
        name: spark
        namespace: '{{ namespace }}'
ingress:
  create: true
  commonAnnotations:
    kubernetes.io/ingress.class: nginx
  httpAnnotations:
    nginx.ingress.kubernetes.io/app-root: /console
  grpcAnnotations:
    nginx.ingress.kubernetes.io/backend-protocol: GRPC
  host: <your-Nebula-URL> # change for the URL you'll use to connect to Nebula
rbac:
  extraRules:
    - apiGroups:
      - ""
      resources:
      - pods
      - services
      - configmaps
      verbs:
      - "*"
    - apiGroups:
      - ""
      resources:
      - serviceaccounts
      verbs:
      - create
      - get
      - list
      - patch
      - update
    - apiGroups:
      - rbac.authorization.k8s.io
      resources:
      - rolebindings
      - roles
      verbs:
      - create
      - get
      - list
      - patch
      - update
    - apiGroups:
      - sparkoperator.k8s.io
      resources:
      - sparkapplications
      verbs:
      - "*"
serviceAccount:
  create: true
  annotations:
    eks.amazonaws.com/role-arn: "<NEBULA_BACKEND_IAM_ARN>"